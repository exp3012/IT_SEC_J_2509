{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/exp3012/IT_SEC_J_2509/blob/main/denselayer_minst.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1eghUREPRdty",
        "outputId": "8efa0664-6498-4777-eba1-b7497c48bd99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "데이터 저장 경로: ./mnist_data_dir\n",
            "MNIST 데이터셋 로드 중...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "학습 이미지 크기: (60000, 28, 28)\n",
            "테스트 이미지 크기: (10000, 28, 28)\n",
            "train 이미지 파일 저장 시작...\n",
            "train 데이터 디렉토리 구조 생성 완료.\n",
            "test 이미지 파일 저장 시작...\n",
            "test 데이터 디렉토리 구조 생성 완료.\n",
            "\n",
            "디렉토리에서 데이터셋 로드 중...\n",
            "Found 60000 files belonging to 10 classes.\n",
            "Found 10000 files belonging to 10 classes.\n",
            "데이터셋 로드 및 전처리 완료. (3채널 RGB 모드)\n",
            "\n",
            "모델 구조 (3채널 입력):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ Flatten_Layer_2352_features     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2352\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mFlatten\u001b[0m)                       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_Hidden_512 (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m1,204,736\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_Hidden_256 (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Output_Layer_10_Classes (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ Flatten_Layer_2352_features     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2352</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                       │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_Hidden_512 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,204,736</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Dense_Hidden_256 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ Output_Layer_10_Classes (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,338,634\u001b[0m (5.11 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,338,634</span> (5.11 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,338,634\u001b[0m (5.11 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,338,634</span> (5.11 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "모델 학습 시작...\n",
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 29ms/step - accuracy: 0.9065 - loss: 0.2980 - val_accuracy: 0.9658 - val_loss: 0.1069\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - accuracy: 0.9711 - loss: 0.0936 - val_accuracy: 0.9637 - val_loss: 0.1217\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.9794 - loss: 0.0655 - val_accuracy: 0.9715 - val_loss: 0.1041\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.9855 - loss: 0.0482 - val_accuracy: 0.9764 - val_loss: 0.1053\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.9868 - loss: 0.0413 - val_accuracy: 0.9743 - val_loss: 0.1141\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 24ms/step - accuracy: 0.9895 - loss: 0.0348 - val_accuracy: 0.9717 - val_loss: 0.1406\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - accuracy: 0.9904 - loss: 0.0309 - val_accuracy: 0.9777 - val_loss: 0.1174\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 22ms/step - accuracy: 0.9909 - loss: 0.0286 - val_accuracy: 0.9778 - val_loss: 0.1192\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - accuracy: 0.9910 - loss: 0.0300 - val_accuracy: 0.9771 - val_loss: 0.1178\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 22ms/step - accuracy: 0.9925 - loss: 0.0241 - val_accuracy: 0.9723 - val_loss: 0.1547\n",
            "\n",
            "테스트 데이터셋으로 최종 모델 평가:\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.9769 - loss: 0.1324\n",
            "\n",
            "최종 테스트 정확도 (Dense Layer Only, 3-Channel): 97.23%\n",
            "3채널(RGB) 이미지로 모델 학습이 완료되었습니다.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from PIL import Image # [수정됨] .convert('RGB')를 사용하기 위해 import 확인\n",
        "\n",
        "# --- 1. 환경 설정 및 데이터 준비 ---\n",
        "# MNIST 데이터를 디렉토리 구조로 저장하기 위한 경로 설정\n",
        "DATA_DIR = './mnist_data_dir'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "print(f\"데이터 저장 경로: {DATA_DIR}\")\n",
        "\n",
        "# 기존 디렉토리 구조가 있다면 삭제하고 새로 생성\n",
        "if os.path.exists(DATA_DIR):\n",
        "    shutil.rmtree(DATA_DIR)\n",
        "os.makedirs(DATA_DIR)\n",
        "\n",
        "# Keras에서 MNIST 데이터셋 로드 (NumPy 배열 형태)\n",
        "print(\"MNIST 데이터셋 로드 중...\")\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(f\"학습 이미지 크기: {x_train.shape}\")\n",
        "print(f\"테스트 이미지 크기: {x_test.shape}\")\n",
        "\n",
        "# 디렉토리에 이미지 파일로 저장하는 함수 (0-9까지의 클래스별 폴더 생성)\n",
        "def save_images_to_directory(images, labels, base_dir, split_name):\n",
        "    # 'train' 또는 'test' 폴더 생성\n",
        "    split_dir = os.path.join(base_dir, split_name)\n",
        "    os.makedirs(split_dir, exist_ok=True)\n",
        "\n",
        "    # 클래스(0~9)별 폴더 생성\n",
        "    for i in range(10):\n",
        "        os.makedirs(os.path.join(split_dir, str(i)), exist_ok=True)\n",
        "\n",
        "    # 이미지 파일 저장\n",
        "    print(f\"{split_name} 이미지 파일 저장 시작...\")\n",
        "    for i in range(len(images)):\n",
        "        label = labels[i]\n",
        "\n",
        "        image_name = f\"{i:05d}.png\"\n",
        "        file_path = os.path.join(split_dir, str(label), image_name)\n",
        "\n",
        "        # [수정됨] 1/3: 흑백(L) 이미지를 3채널(RGB)로 변환하여 저장\n",
        "        # 흑백 이미지를 8비트 그레이스케일로 변환\n",
        "        img = Image.fromarray(images[i].astype(np.uint8))\n",
        "        # 1채널 이미지를 R,G,B 채널에 복제하여 3채널 이미지로 변환\n",
        "        img = img.convert('RGB')\n",
        "        img.save(file_path)\n",
        "\n",
        "    print(f\"{split_name} 데이터 디렉토리 구조 생성 완료.\")\n",
        "\n",
        "\n",
        "save_images_to_directory(x_train, y_train, DATA_DIR, 'train')\n",
        "save_images_to_directory(x_test, y_test, DATA_DIR, 'test')\n",
        "\n",
        "\n",
        "# --- 2. 'image_dataset_from_directory'를 이용한 데이터 로드 및 전처리 ---\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = (28, 28)\n",
        "\n",
        "print(\"\\n디렉토리에서 데이터셋 로드 중...\")\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=os.path.join(DATA_DIR, 'train'),\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    interpolation='bilinear',\n",
        "    color_mode='rgb', # [수정됨] 2/3: 'grayscale'이 아닌 'rgb' (3채널)로 로드\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory=os.path.join(DATA_DIR, 'test'),\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        "    image_size=IMAGE_SIZE,\n",
        "    interpolation='bilinear',\n",
        "    color_mode='rgb', # [수정됨] 2/3: 'grayscale'이 아닌 'rgb' (3채널)로 로드\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# 데이터 정규화 함수 (픽셀 값을 0과 1 사이로)\n",
        "# (이미지가 (28, 28, 3)으로 로드되지만 정규화 방식은 동일함)\n",
        "def normalize(image, label):\n",
        "    # 픽셀 값을 0-255에서 0-1로 변환합니다.\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    return image, label\n",
        "\n",
        "# 데이터셋에 정규화 적용 및 캐싱/프리페치 설정 (학습 속도 향상)\n",
        "train_ds = train_ds.map(normalize).cache().prefetch(buffer_size=tf.data.AUTOTUNE) # Corrected typo\n",
        "val_ds = val_ds.map(normalize).cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "print(\"데이터셋 로드 및 전처리 완료. (3채널 RGB 모드)\")\n",
        "\n",
        "\n",
        "# --- 3. Dense Layer만으로 구성된 모델 정의 ---\n",
        "\n",
        "model = Sequential([\n",
        "    # [수정됨] 3/3: 입력 shape을 (28, 28, 3)으로 변경\n",
        "    # (28, 28) 크기의 3채널(RGB) 이미지를\n",
        "    # (28 * 28 * 3 = 2352) 크기의 1차원 벡터로 '펼칩니다'.\n",
        "    Flatten(input_shape=(28, 28, 3), name='Flatten_Layer_2352_features'),\n",
        "\n",
        "    # 은닉층 1: 512개의 뉴런을 가진 Dense Layer\n",
        "    Dense(512, activation='relu', name='Dense_Hidden_512'),\n",
        "\n",
        "    # 은닉층 2: 256개의 뉴런을 가진 Dense Layer (추가적인 복잡도 학습)\n",
        "    Dense(256, activation='relu', name='Dense_Hidden_256'),\n",
        "\n",
        "    # 출력층: 10개의 뉴런 (0~9의 숫자), Softmax 활성화 함수 (확률 분포 출력)\n",
        "    Dense(10, activation='softmax', name='Output_Layer_10_Classes')\n",
        "])\n",
        "\n",
        "# --- 4. 모델 컴파일 및 학습 ---\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"\\n모델 구조 (3채널 입력):\")\n",
        "model.summary() # Flatten 레이어의 Param이 0, Dense_Hidden_512의 Param이 (2352+1)*512 임을 확인\n",
        "\n",
        "print(\"\\n모델 학습 시작...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=10 # 에포크 수: 10회 학습\n",
        ")\n",
        "\n",
        "# --- 5. 최종 평가 ---\n",
        "print(\"\\n테스트 데이터셋으로 최종 모델 평가:\")\n",
        "loss, accuracy = model.evaluate(val_ds)\n",
        "\n",
        "print(f\"\\n최종 테스트 정확도 (Dense Layer Only, 3-Channel): {accuracy*100:.2f}%\")\n",
        "print(\"3채널(RGB) 이미지로 모델 학습이 완료되었습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. [추가] 학습 완료된 모델 저장하기 ---\n",
        "# (이 부분은 이전과 동일하게 모델을 저장합니다)\n",
        "MODEL_SAVE_PATH = './my_mnist_dense_model.keras'\n",
        "\n",
        "print(f\"\\n학습된 모델을 '{MODEL_SAVE_PATH}' 경로에 저장합니다...\")\n",
        "model.save(MODEL_SAVE_PATH)\n",
        "print(\"모델 저장 완료.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAAyc9J-T2-m",
        "outputId": "fcdba361-26b8-44a7-d193-69fa63f80448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "학습된 모델을 './my_mnist_dense_model.keras' 경로에 저장합니다...\n",
            "모델 저장 완료.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. [요청대로 수정] 시각화 X, 컬러(RGB) 로드 ---\n",
        "\n",
        "# from matplotlib.pyplot as plt # [요청] 시각화 제거로 import 안 함\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- 1. 모델 로드 ---\n",
        "print(f\"\\n'{MODEL_SAVE_PATH}' 경로에서 모델을 로드합니다...\")\n",
        "loaded_model = tf.keras.models.load_model(MODEL_SAVE_PATH)\n",
        "print(\"모델 로드 완료.\")\n",
        "\n",
        "\n",
        "# --- 2. 예측할 이미지 파일 경로 지정 ---\n",
        "# !!! 사용자가 예측하려는 파일의 경로를 여기에 직접 입력하세요 !!!\n",
        "my_img = \"minst_1_2.png\" # (예: \"my_cat.jpg\")\n",
        "\n",
        "# (테스트용: 1.png가 없다면 임시로 하나 복사합니다.)\n",
        "if not os.path.exists(my_img) and os.path.exists(os.path.join(DATA_DIR, 'test', '7', '00000.png')):\n",
        "    print(f\"[테스트용 알림] '{my_img}'가 없어 임시 파일을 복사합니다. 예측할 파일을 직접 업로드하세요.\")\n",
        "    shutil.copy(os.path.join(DATA_DIR, 'test', '7', '00000.png'), my_img)\n",
        "\n",
        "\n",
        "# --- 3. 파일 존재 확인 및 예측 수행 ---\n",
        "if not os.path.exists(my_img):\n",
        "    print(f\"\\n[오류] '{my_img}' 파일을 찾을 수 없습니다!\")\n",
        "    print(f\"Colab의 왼쪽 '파일' 탭에 '{my_img}' 파일을 업로드하거나 경로를 확인해주세요.\")\n",
        "else:\n",
        "    print(f\"\\n--- 지정된 파일로 예측 시작: {my_img} ---\")\n",
        "\n",
        "    # 1. 이미지 로드 (28x28 크기, 컬러(RGB)로 로드)\n",
        "    img = load_img(\n",
        "        my_img,\n",
        "        # color_mode='grayscale', # [요청] 흑백 처리 제거 (기본값 'rgb')\n",
        "        target_size=(28, 28)\n",
        "    )\n",
        "\n",
        "    # 2. 이미지를 NumPy 배열로 변환\n",
        "    # (흑백 처리를 뺐으므로 shape은 (28, 28, 3)이 됩니다)\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    # 3. 정규화 (0~1 사이로)\n",
        "    img_array = img_array / 255.0\n",
        "\n",
        "    # 4. 배치 차원 추가\n",
        "    # (shape: (1, 28, 28, 3))\n",
        "    img_for_prediction = tf.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # 5. 예측 수행 (채널 수가 맞지 않아 오류가 발생할 것입니다)\n",
        "    try:\n",
        "        prediction_scores = loaded_model.predict(img_for_prediction)\n",
        "\n",
        "        # 6. 결과 해석\n",
        "        predicted_label = np.argmax(prediction_scores[0])\n",
        "        confidence = 100 * np.max(prediction_scores[0])\n",
        "\n",
        "        print(f\"  모델 예측 결과: {predicted_label}\")\n",
        "        print(f\"  신뢰도: {confidence:.2f}%\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(\"\\n\" + \"=\"*30)\n",
        "        print(\"[예측 오류 발생!]\")\n",
        "        print(\"  (이것은 정상적인 오류입니다)\")\n",
        "        print(f\"  오류 원인: {e}\")\n",
        "        print(\"\\n[해설]\")\n",
        "        print(f\"  훈련된 모델의 입력 shape: {loaded_model.input_shape} (흑백/1채널)\")\n",
        "        print(f\"  현재 입력된 이미지 shape: {img_for_prediction.shape} (컬러/3채널)\")\n",
        "        print(\"  -> 모델을 흑백(1채널)으로 학습시켰기 때문에 컬러(3채널) 이미지를 예측할 수 없습니다.\")\n",
        "        print(\"=\"*30)\n",
        "\n",
        "    # 7. 예측 결과 시각화\n",
        "    # [요청] \"이미지 없는 상황\" -> 시각화 코드(plt.show()) 완전 제거"
      ],
      "metadata": {
        "id": "-Gp3c9SjlvDm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}